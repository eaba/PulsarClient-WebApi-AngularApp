#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Flag to control whether to run initialize job
initialize: true

## clusterDomain as defined for your k8s cluster
clusterDomain: cluster.local
###
### K8S Settings
###

## Namespace to deploy pulsar
# NOTE: Make the default namespace as empty. So it will fallback to use the namespace used for installing the helm
#       chart. Helm does not position it self as a namespace manager, as namespaces in kubernetes are considered as
#       a higher control structure that is not part of the application.
namespace: "pulsar"
namespaceCreate: false

###
### Global Settings
###

global:
  ## Reference to one or more secrets to be used when pulling images
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  imagePullSecrets: []
  ## - name: image-pull-secret

## Pulsar Metadata Prefix
##
## By default, pulsar stores all the metadata at root path.
## You can configure to have a prefix (e.g. "/my-pulsar-cluster").
## If you do so, all the pulsar and bookkeeper metadata will
## be stored under the provided path
metadataPrefix: ""

## Persistence
##
## If persistence is enabled, components that have state will
## be deployed with PersistentVolumeClaims, otherwise, for test
## purposes, they will be deployed with emptyDir
##
## This is a global setting that is applied to all components.
## If you need to disable persistence for a component,
## you can set the `volume.persistence` setting to `false` for
## that component.
volumes:
  persistence: true

## AntiAffinity
##
## Flag to enable and disable `AntiAffinity` for all components.
## This is a global setting that is applied to all components.
## If you need to disable AntiAffinity for a component, you can set
## the `affinity.anti_affinity` settings to `false` for that component.
##
## To enable anti_affintiy for a zone, enable `affinity.zone_anti_affinity: true`.
## This can be disabled for each component as well
affinity:
  anti_affinity: true
  zone_anti_affinity: false
  zone_anti_affinity_weight: 100

## Components
##
## Control what components of Apache Pulsar to deploy for the cluster
components:  
  # zookeeper
  zookeeper: true
  # bookkeeper
  bookkeeper: true
  # bookkeeper - autorecovery
  autorecovery: true
  # broker
  broker: false
  # functions
  functions: false
  # proxy
  proxy: false
  # toolset
  toolset: false
  # pulsar sql
  sql_worker: true
  # pulsar manager
  pulsar_manager: false

## which extra components to deploy (Deprecated)
extra:
  # Pulsar proxy
  proxy: false
  # Bookkeeper auto-recovery
  autoRecovery: true
  # Pulsar dashboard
  # Deprecated
  # Replace pulsar-dashboard with pulsar-manager
  dashboard: false
  # pulsar manager
  pulsar_manager: false
  # Configure Kubernetes runtime for Functions
  functionsAsPods: false

# default image tag for pulsar images
# uses chart's appVersion when unspecified
defaultPulsarImageTag:
## Images
##
## Control what images to use for each component
images:
  zookeeper:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
    customTools:
      backup:
        repository: "streamnative/pulsar-metadata-tool"
        tag: "2.10.4.7"
      restore:
        repository: "streamnative/pulsar-metadata-tool"
        tag: "2.10.4.7"
  bookie:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
  presto:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
    exporter:
      repository: bitnami/jmx-exporter
      tag: "0.17.0"
      pullPolicy: IfNotPresent
  autorecovery:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
  broker:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
  proxy:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
  functions:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
  function_worker:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
  # NOTE: allow overriding the toolset image
  toolset:
    repository: apachepulsar/pulsar-all
    tag: "3.0.0"
    pullPolicy: IfNotPresent
    kafka:
      repository: confluentinc/cp-kafka
      tag: "7.3.1"
      pullPolicy: IfNotPresent
    busybox:
      repository: busybox
      tag: "1.36.1-uclibc"
      pullPolicy: IfNotPresent  
  pulsar_manager:
    repository: apachepulsar/pulsar-manager
    tag: v0.4.0
    pullPolicy: IfNotPresent
    hasCommand: false

## TLS
## templates/tls-certs.yaml
##
## The chart is using cert-manager for provisioning TLS certs for
## brokers and proxies.
tls:
  enabled: false
  # common settings for generating certs
  common:
    # 90d
    duration: 2160h
    # 15d
    renewBefore: 360h
    subject:
      organizations:
      - pulsar
    privateKey:
      size: 4096
      algorithm: RSA
      encoding: PKCS8
    caSecretName:
  # settings for generating certs for proxy
  proxy:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-proxy
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
    untrustedCa: true
  # settings for generating certs for proxy
  pulsar_detector:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-pulsar-detector
  # settings for generating certs for broker
  broker:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-broker
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
    trustCertsEnabled: false
    gateway:
      # name of chart generated certificate
      cert_name: tls-broker-gateway
      # specify name of secret contain certificate if using pre-generated certificate
      certSecretName:
      trustCertsEnabled: false
  functions:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-function
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
    trustCertsEnabled: false
  # settings for generating certs for bookies
  bookie:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-bookie
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
  # settings for generating certs for zookeeper
  zookeeper:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-zookeeper
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
  # settings for generating certs for recovery
  autorecovery:
    cert_name: tls-recovery
    # name of chart generated certificate
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
  # settings for generating certs for toolset
  toolset:
    cert_name: tls-toolset
    # name of chart generated certificate
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
  presto:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-presto
    # specify name of secret contain certificate if using pre-generated certificate
    certSecretName:
    trustCertsEnabled: false
    # use "changeit" as password if not set passwordSecretRef
    # passwordSecretRef:
    #   key: password
    #   name: ""
  streamnative_console:
    enabled: false
    # name of chart generated certificate
    cert_name: tls-streamnative-console

# Enable or disable broker authentication and authorization.
auth:
  authentication:
    enabled: true
    provider: "jwt"
    jwt:
      enabled: false
      # Enable JWT authentication
      # If the token is generated by a secret key, set the usingSecretKey as true.
      # If the token is generated by a private key, set the usingSecretKey as false.
      usingSecretKey: false
    tls:
      enabled: false
  authorization:
    enabled: true
  superUsers:
    # broker to broker communication, broker superUserRoles
    broker: "admin,proxy-admin,broker-admin,admin-approle,pulsar-manager-admin"
    # if it is enabled, the roles configured in the proxy will not be used by the client
    proxyRolesEnabled: false
    # if proxyRolesEnabled is set to true, this role is only available to proxy connection brokers, please make sure the client does not use this role
    proxyRoles: proxy-admin
    # proxy to broker communication, proxy superUserRoles
    proxy: "admin,proxy-admin,admin-approle,pulsar-manager-admin"
    # websocket proxy to broker communication
    websocket: "admin,ws-admin,admin-approle,pulsar-manager-admin"
    # pulsar-admin client to broker/proxy communication
    client: "admin"
    # streamnative-console
    streamnative-console: "super,pulsar-manager-admin"
  # Enable vault based authentication
  vault:
    enabled: true
  oauth:
    enabled: false
    oauthIssuerUrl: "https://login.microsoftonline.com/your-tenant-id/v2.0"
    oauthAudience: "your-application-id"
    oauthSubjectClaim: "oid"
    oauthScopeClaim: "scp"
    oauthAuthzRoleClaim: "roles"
    # The name of the role when creating the application
    oauthAuthzAdminRole: ""
    # brokerClientCredential: ""
    # brokerClientAuthenticationPlugin: org.apache.pulsar.client.impl.auth.oauth2.AuthenticationOAuth2
    # brokerClientAuthenticationParameters: ""
    # Create secret by use kubectl
    # kubectl create secret generic my-secret \
    # --from-literal=broker_client_credential.json=$(echo -n '{"client_id":"your-client-id","client_secret":"your-client-secret","issuer_url":"http://your-issuer-url/"}') \
    # -n your-namespace
    # brokerClientCredentialSecret: "my-secret"
    # brokerClientAuthenticationParameters: {"privateKey":"file:///mnt/secrets/broker_client_credential.json", "issuerUrl":"https://login.microsoftonline.com/your-tenant-id/v2.0", "audience":"your-audience", "scope":"your-scope"}
    authenticationProvider: "io.streamnative.pulsar.broker.authentication.AuthenticationProviderOAuth"
    authorizationProvider: "io.streamnative.pulsar.broker.authorization.AuthorizationProviderOAuth"


######################################################################
# External dependencies
######################################################################

## cert-manager
## templates/tls-cert-issuer.yaml
##
## Cert manager is used for automatically provisioning TLS certificates
## for components within a Pulsar cluster
certs:
  internal_issuer:
    enabled: false
    component: internal-cert-issuer
    # Type options are selfsigning, secret, custom
    #  selfsigning doesn't need another configure
    #  secret depends on certs.issuers.secret.secretName
    #  custom depends on certs.issuers.custom
    type: selfsigning
  public_issuer:
    enabled: false
    component: public-cert-issuer
    type: acme

  istio_internal_issuer:
    enabled: false
    component: istio-internal-cert-issuer
    # Type options are selfsigning, secret
    #  selfsigning doesn't need another configure
    #  secret depends on certs.issuers.secret.secretName
    #  custom depends on certs.issuers.custom
    type: selfsigning

  istio_public_issuer:
    enabled: false
    component: istio-public-cert-issuer
    type: acme

  issuers:
    selfsigning:
    secret:
      secretName: ca-keypair
    custom: {}
    acme:
      # You must replace this email address with your own.
      # Let's Encrypt will use this to contact you about expiring
      # certificates, and issues related to your account.
      email: contact@example.local
      # change this to production endpoint once you successfully test it
      # server: https://acme-v02.api.letsencrypt.org/directory
      server: https://acme-staging-v02.api.letsencrypt.org/directory
      solver: clouddns
      solvers:
        clouddns:
          # TODO: add a link about how to configure this section
          project: "[YOUR GCP PROJECT ID]"
          serviceAccountSecretRef:
            name: "[NAME OF SECRET]"
            key: "[KEY OF SECRET]"
        # route53:
        #   region: "[ROUTE53 REGION]"
        #   secretAccessKeySecretRef:
        #     name: "[NAME OF SECRET]"
        #     key: "[KEY OF SECRET]"
        #   role: "[ASSUME A ROLE]"
        # cloudflare:
        #   email: "[YOUR ACCOUNT EMAIL]"
        #   apiTokenSecretRef:
        #     name: "[NAME OF SECRET]"
        #     key: "[KEY OF SECRET]"
  lets_encrypt:
    ca_ref:
      secretName: "[SECRET STORES lets encrypt CA]"
      keyName: "[KEY IN THE SECRET STORES let encrypt CA]"

## External DNS
## templates/external-dns.yaml
## templates/external-dns-rbac.yaml
##
## External DNS is used for synchronizing exposed/st Ingresses with DNS providers
external_dns:
  enabled: false
  component: external-dns
  policy: upsert-only
  registry: txt
  owner_id: pulsar
  domain_filter: pulsar.example.local
  provider: google
  providers:
    google:
      # project: external-dns-test
      project: "[GOOGLE PROJECT ID]"
    aws:
      zoneType: public
  serviceAcct:
    annotations: {}
  securityContext: {}
  extraMounts: []
  extraEnv: []


## Domain requested from External DNS
domain:
  enabled: false
  suffix: test.pulsar.example.local

## Ingresses for exposing Pulsar services
ingress:
  ## templates/proxy-service-ingress.yaml
  ##
  ## Ingresses for exposing pulsar service publicly
  proxy:
    enabled: false
    httpPortOverride: ""
    ## When these conditions are satisfied
    ##   1. Values.ingress.proxy.type == LoadBalancer
    ##   2. Values.tls.enabled == false and Values.tls.proxy.enabled == false
    ## If tls is enabled, it will expose proxy service port with TLS port 443 and 6651
    ## otherwise, the ports will be 8080 and 6650 as default
    tls:
      enabled: true
    ## Type options are LoadBalancer, ClusterIP, IstioGateway
    ##   IstioGateway means using Istio Ingress Gateway, available when Istio enabled
    type: LoadBalancer
    annotations: {}
    extraSpec: {}
    # external_domain: your.external.proxy.domain
  ## templates/broker-service-ingress.yaml
  ##
  ## Ingresses for exposing pulsar service publicly
  broker:
    enabled: false
    type: LoadBalancer
    annotations: {}
    extraSpec: {}
    # Set external domain of the broker
    # external_domain: your.external.broker.domain
  ## templates/presto-service-ingress.yaml
  ##
  ## Ingresses for exposing presto service publicly
  presto:
    enabled: false
    # Set external domain of the presto
    # external_domain: your.external.presto.domain
    tls:
      enabled: true
    type: LoadBalancer
    annotations: {}
    extraSpec: {}
    ports:
      http: 80
      https: 443
  ## templates/control-center-ingress.yaml
  ##
  ## Ingresses for exposing monitoring/management services publicly
  controller:
    enabled: false
    rbac: true
    component: nginx-ingress-controller
    replicaCount: 1
    # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
    tolerations: []
    gracePeriod: 300
    annotations: {}
    ports:
      http: 80
      https: 443
    # flag whether to terminate the tls at the loadbalancer level
    tls:
      termination: false
  control_center:
    enabled: true
    component: control-center
    endpoints:
      grafana: true
      prometheus: false
      alertmanager: false
    # Set external domain of the load balancer of ingress controller
    # external_domain: your.external.control.center.domain
    external_domain_scheme: https://
    # external_domain_scheme should same with actual situation
    tls:
      enabled: false
    annotations: {}


######################################################################
# Below are settings for each component
######################################################################

## Common properties applied to pulsar components
common:
  extraInitContainers: []


## Pulsar: Zookeeper cluster
## templates/zookeeper-statefulset.yaml
##
zookeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: zookeeper
  # the number of zookeeper servers to run. it should be an odd number larger than or equal to 3.
  replicaCount: 1
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  # This is how prometheus discovers this component
  podMonitor:
    enabled: true
    interval: 10s
    scrapeTimeout: 10s
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  ports:
    http: 8000
    client: 2181
    clientTls: 2281
    follower: 2888
    leaderElection: 3888
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 20
      periodSeconds: 30
      timeoutSeconds: 30
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 20
      periodSeconds: 30
      timeoutSeconds: 30
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 20
      periodSeconds: 30
      timeoutSeconds: 30
  affinity:
    anti_affinity: true
    anti_affinity_topology_key: kubernetes.io/hostname
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guranentee
    type: requiredDuringSchedulingIgnoredDuringExecution
  annotations: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  # extraVolumes and extraVolumeMounts allows you to mount other volumes
  # Example Use Case: mount ssl certificates
  # extraVolumes:
  #   - name: ca-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: ca-certs
  # extraVolumeMounts:
  #   - name: ca-certs
  #     mountPath: /certs
  #     readOnly: true
  extraVolumes: []
  extraVolumeMounts: []
  # Ensures 2.10.0 non-root docker image works correctly.
  securityContext:
    fsGroup: 0
    fsGroupChangePolicy: "OnRootMismatch"
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    data:
      name: data
      size: 20Gi
      local_storage: true
      ## If you already have an existent storage class and want to reuse it, you can specify its name with the option below
      ##
      # storageClassName: existent-storage-class
      #
      ## Instead if you want to create a new storage class define it below
      ## If left undefined no storage class will be defined along with PVC
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
      ## If you want to bind static persistent volumes via selectors, e.g.:
      # selector:
        # matchLabels:
        # app: pulsar-zookeeper
      selector: {}
  # External zookeeper server list in case of global-zk list to create zk cluster across zk deployed on different clusters/namespaces
  # Example value: "us-east1-pulsar-zookeeper-0.us-east1-pulsar-zookeeper.us-east1.svc.cluster.local:2888:3888,us-east1-pulsar-zookeeper-1.us-east1-pulsar-zookeeper.us-east1.svc.cluster.local:2888:3888,us-east1-pulsar-zookeeper-2.us-east1-pulsar-zookeeper.us-east1.svc.cluster.local:2888:3888,us-west1-pulsar-zookeeper-0.us-west1-pulsar-zookeeper.us-west1.svc.cluster.local:2888:3888,us-west1-pulsar-zookeeper-1.us-west1-pulsar-zookeeper.us-west1.svc.cluster.local:2888:3888,us-west1-pulsar-zookeeper-2.us-west1-pulsar-zookeeper.us-west1.svc.cluster.local:2888:3888"
  externalZookeeperServerList: ""
  ## Zookeeper configmap
  ## templates/zookeeper-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      -Xms64m -Xmx128m
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -Dcom.sun.management.jmxremote
      -Djute.maxbuffer=10485760
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:+DisableExplicitGC
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
  ## Add a custom command to the start up process of the zookeeper pods (e.g. update-ca-certificates, jvm commands, etc)
  additionalCommand:
  ## Zookeeper service
  ## templates/zookeeper-service.yaml
  ##
  service:
    annotations: {}
  ## Zookeeper PodDisruptionBudget
  ## templates/zookeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1


## Pulsar: Bookkeeper cluster
## templates/bookkeeper-statefulset.yaml
##
bookkeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: bookie
  ## BookKeeper Cluster Initialize
  ## templates/bookkeeper-cluster-initialize.yaml
  metadata:
    ## Set the resources used for running `bin/bookkeeper shell initnewcluster`
    ##
    resources:
      # requests:
        # memory: 4Gi
        # cpu: 2
  replicaCount: 4
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  # This is how prometheus discovers this component
  podMonitor:
    enabled: true
    interval: 10s
    scrapeTimeout: 10s
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  ports:
    http: 8000
    bookie: 3181
    statestore: 4181
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 60
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 5
    readiness:
      enabled: true
      failureThreshold: 60
      initialDelaySeconds: 10
      periodSeconds: 30
      timeoutSeconds: 5
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 5
  affinity:
    anti_affinity: true
    anti_affinity_topology_key: kubernetes.io/hostname
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guranentee
    type: requiredDuringSchedulingIgnoredDuringExecution
  annotations: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
  # extraVolumes and extraVolumeMounts allows you to mount other volumes
  # Example Use Case: mount ssl certificates
  # extraVolumes:
  #   - name: ca-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: ca-certs
  # extraVolumeMounts:
  #   - name: ca-certs
  #     mountPath: /certs
  #     readOnly: true
  extraVolumes: []
  extraVolumeMounts: []
  # Ensures 2.10.0 non-root docker image works correctly.
  securityContext:
    fsGroup: 0
    fsGroupChangePolicy: "OnRootMismatch"
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    journal:
      name: journal
      size: 10Gi
      local_storage: true
      ## If you already have an existent storage class and want to reuse it, you can specify its name with the option below
      ##
      # storageClassName: existent-storage-class
      #
      ## Instead if you want to create a new storage class define it below
      ## If left undefined no storage class will be defined along with PVC
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
      ## If you want to bind static persistent volumes via selectors, e.g.:
      # selector:
        # matchLabels:
        # app: pulsar-bookkeeper-journal
      selector: {}
      useMultiVolumes: false
      multiVolumes:
        - name: journal0
          size: 10Gi
          # storageClassName: existent-storage-class
          mountPath: /pulsar/data/bookkeeper/journal0
        - name: journal1
          size: 10Gi
          # storageClassName: existent-storage-class
          mountPath: /pulsar/data/bookkeeper/journal1
    ledgers:
      name: ledgers
      size: 50Gi
      local_storage: true
      # storageClassName:
      # storageClass:
        # ...
      # selector:
        # ...
      useMultiVolumes: false
      multiVolumes:
        - name: ledgers0
          size: 10Gi
          # storageClassName: existent-storage-class
          mountPath: /pulsar/data/bookkeeper/ledgers0
        - name: ledgers1
          size: 10Gi
          # storageClassName: existent-storage-class
          mountPath: /pulsar/data/bookkeeper/ledgers1

    ## use a single common volume for both journal and ledgers
    useSingleCommonVolume: false
    common:
      name: common
      size: 60Gi
      local_storage: true
      # storageClassName:
      # storageClass: ## this is common too
        # ...
      # selector:
        # ...

  ## Bookkeeper configmap
  ## templates/bookkeeper-configmap.yaml
  ##
  configData:
    # we use `bin/pulsar` for starting bookie daemons
    PULSAR_MEM: >
      -Xms128m
      -Xmx256m
      -XX:MaxDirectMemorySize=256m
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
    # configure the memory settings based on jvm memory settings
    dbStorage_writeCacheMaxSizeMb: "32"
    dbStorage_readAheadCacheMaxSizeMb: "32"
    dbStorage_rocksDB_writeBufferSizeMB: "8"
    dbStorage_rocksDB_blockCacheSize: "8388608"
  ## Add a custom command to the start up process of the bookie pods (e.g. update-ca-certificates, jvm commands, etc)
  additionalCommand:
  ## Bookkeeper Service
  ## templates/bookkeeper-service.yaml
  ##
  service:
    spec:
      publishNotReadyAddresses: true
  ## Bookkeeper PodDisruptionBudget
  ## templates/bookkeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper AutoRecovery
## templates/autorecovery-statefulset.yaml
##
autorecovery:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: recovery
  replicaCount: 1
  # This is how prometheus discovers this component
  podMonitor:
    enabled: true
    interval: 10s
    scrapeTimeout: 10s
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  ports:
    http: 8000
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  affinity:
    anti_affinity: true
    anti_affinity_topology_key: kubernetes.io/hostname
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guranentee
    type: requiredDuringSchedulingIgnoredDuringExecution
  annotations: {}
  # tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 64Mi
      cpu: 0.05
  ## Bookkeeper auto-recovery configmap
  ## templates/autorecovery-configmap.yaml
  ##
  configData:
    BOOKIE_MEM: >
      -Xms64m -Xmx64m
    PULSAR_PREFIX_useV2WireProtocol: "true"

## Pulsar Zookeeper metadata. The metadata will be deployed as
## soon as the last zookeeper node is reachable. The deployment
## of other components that depends on zookeeper, such as the
## bookkeeper nodes, broker nodes, etc will only start to be
## deployed when the zookeeper cluster is ready and with the
## metadata deployed
pulsar_metadata:
  component: pulsar-init
  image:
    # the image used for running `pulsar-cluster-initialize` job
    repository: apachepulsar/pulsar-all
    # uses defaultPulsarImageTag when unspecified
    tag:
    pullPolicy: IfNotPresent
  ## set an existing configuration store
  # configurationStore:
  configurationStoreMetadataPrefix: ""
  configurationStorePort: 2181
  ## optional you can specify a nodeSelector for all init jobs (pulsar-init & bookkeeper-init)
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool

  ## optional, you can provide your own zookeeper metadata store for other components
  # to use this, you should explicit set components.zookeeper to false
  #
  # userProvidedZookeepers: "zk01.example.com:2181,zk02.example.com:2181"

  ## optional, you can specify where to run pulsar-cluster-initialize job
  # nodeSelector:

# Can be used to run extra commands in the initialization jobs e.g. to quit istio sidecars etc.
extraInitCommand: ""

## Pulsar: Broker cluster
## templates/broker-statefulset.yaml
##
broker:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: broker
  replicaCount: 3
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics: ~
  # This is how prometheus discovers this component
  podMonitor:
    enabled: true
    interval: 10s
    scrapeTimeout: 10s
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  ports:
    http: 8080
    https: 8443
    pulsar: 6650
    pulsarssl: 6651
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
  affinity:
    anti_affinity: true
    anti_affinity_topology_key: kubernetes.io/hostname
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guranentee
    type: preferredDuringSchedulingIgnoredDuringExecution
  annotations: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
  # extraVolumes and extraVolumeMounts allows you to mount other volumes
  # Example Use Case: mount ssl certificates
  # extraVolumes:
  #   - name: ca-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: ca-certs
  # extraVolumeMounts:
  #   - name: ca-certs
  #     mountPath: /certs
  #     readOnly: true
  extraVolumes: []
  extraVolumeMounts: []
  extreEnvs: []
#    - name: POD_NAME
#      valueFrom:
#        fieldRef:
#          apiVersion: v1
#          fieldPath: metadata.name
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      -Xms128m -Xmx256m -XX:MaxDirectMemorySize=256m
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
    managedLedgerDefaultEnsembleSize: "1"
    managedLedgerDefaultWriteQuorum: "1"
    managedLedgerDefaultAckQuorum: "1"
    PULSAR_PREFIX_systemTopicEnabled: "true"
    PULSAR_PREFIX_topicLevelPoliciesEnabled: "true"
  ## Add a custom command to the start up process of the broker pods (e.g. update-ca-certificates, jvm commands, etc)
  additionalCommand:
  ## Broker service
  ## templates/broker-service.yaml
  ##
  service:
    annotations: {}
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1
  ### Broker service account
  ## templates/broker-service-account.yaml
  service_account:
    annotations: {}

## Pulsar: Functions Worker
## templates/function-worker-configmap.yaml
##
functions:
  component: functions-worker
  useBookieAsStateStore: false
  ## Pulsar: Functions Worker ClusterRole or Role
  ## templates/broker-rbac.yaml
  # Default is false which deploys functions with ClusterRole and ClusterRoleBinding at the cluster level
  # Set to true to deploy functions with Role and RoleBinding inside the specified namespace
  rbac:
    limit_to_namespace: false

proxy:
  # pulsarProxy indicates the setup for the CR(CustomResource) of the PulsarProxy
  pulsarProxy:
    # labels that will be added on the PulsarProxy CR only.
    labels: {}
    # annotations that will be added on the PulsarProxy CR only.
    annotations: {}

  # The field logConfig can be used to change the log level and log format of pods.
  # The logConfig field is optional. If it is not specified, the component will use the default log configuration /pulsar/conf/log4j2.yaml.
  # If it is specified will dynamically change the log level and log format of the component by changing the CR.
  logConfig:
  # The level field can be used to change the log level of the component. The value can be one of TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF.
    level: "INFO"
  # The format field can be used to change the log format of the component. The value can be one of json, text.
    format: "text"
  # The template field can totally change the log config of the component. The value is a string, which is the content of the log config file.
    template: {}

  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: proxy
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  affinity:
    anti_affinity: true
    zone_anti_affinity: true
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guranentee
    type: preferredDuringSchedulingIgnoredDuringExecution
  topologySpreadConstraints: []
  labels: {}
  annotations: {}
  securityContext:
    runAsNonRoot: true
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: "512Mi"
      cpu: "0.2"
    # limits:
    #   memory: "512Mi"
    #   cpu: "0.8"
  # extra environment variable to define for the containers
  extraEnv: []
  # extra secrets to mount for the pods
  # extraSecretRefs:
  # - mountPath: /path/to/mount
  #   secretName: "[secret name]"
  extraSecretRefs: []
  # Definition of the serviceAccount used to run proxies.
  serviceAccount:
    # Specifies whether to use a service account to run this component
    use: true
    # Specifies whether a service account should be created
    create: true
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    # Extra annotations for the serviceAccount definition. This can either be
    # YAML or a YAML-formatted multi-line templated string map of the
    # annotations to apply to the serviceAccount.
    annotations: {}
  websocket:
    component: websocket
    enabled: false
  brokerServiceURL: ""
  brokerWebServiceURL: ""
  brokerServiceURLTLS: ""
  brokerWebServiceURLTLS: ""
  authenticateMetricsEndpoint:
    enabled: true
  jvm:
    # Pulsar Operator will automatically generate the JVM memory options based on the Pod memory size. You can override this by entering a new value.
    memoryOptions: []
    gcOptions:
    - >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
    extraOptions: []
    gcLoggingOptions: []
  configData: {}
  ## Proxy service
  ## templates/proxy-service.yaml
  ##
  ports:
    http: 8080
    https: 443
    pulsar: 6650
    pulsarssl: 6651
    websocket: 9090
    websockettls: 9443
  ## Operator Controller
  ## templates/broker-cluster.yaml
  operator:
    adopt_existing: false
    updatePolicy: >
      - finalizers
  ## Retention Policy
  resourcePolicy:
    keep: false
  readPublicKeyFromFile: false
  publicKeyPath: ""
  publicKeySecret: ""
  usePodIPAsAdvertisedAddress: false

## Pulsar ToolSet
## templates/toolset-deployment.yaml
##
toolset:
  component: toolset
  useProxy: true
  replicaCount: 1
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  # extraVolumes and extraVolumeMounts allows you to mount other volumes
  # Example Use Case: mount ssl certificates
  # extraVolumes:
  #   - name: ca-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: ca-certs
  # extraVolumeMounts:
  #   - name: ca-certs
  #     mountPath: /certs
  #     readOnly: true
  extraVolumes: []
  extraVolumeMounts: []
  ## Bastion configmap
  ## templates/bastion-configmap.yaml
  ##
  configData:
    PULSAR_MEM: >
      -Xms64M
      -Xmx128M
      -XX:MaxDirectMemorySize=128M
  ## Add a custom command to the start up process of the toolset pods (e.g. update-ca-certificates, jvm commands, etc)
  additionalCommand:

## Components Stack: pulsar_manager
## templates/pulsar-manager.yaml
##
pulsar_manager:
  component: pulsar-manager
  replicaCount: 1
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  # nodeSelector:
  # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  configData:
    REDIRECT_HOST: "http://127.0.0.1"
    REDIRECT_PORT: "9527"
    DRIVER_CLASS_NAME: org.postgresql.Driver
    URL: jdbc:postgresql://127.0.0.1:5432/pulsar_manager
    LOG_LEVEL: DEBUG
    ## If you enabled authentication support
    ## JWT_TOKEN: <token>
    ## SECRET_KEY: data:base64,<secret key>
  ## Pulsar manager service
  ## templates/pulsar-manager-service.yaml
  ##
  service:
    type: LoadBalancer
    port: 9527
    targetPort: 9527
    annotations: {}
  ## Pulsar manager ingress
  ## templates/pulsar-manager-ingress.yaml
  ##
  ingress:
    enabled: false
    annotations: {}
    ingressClassName: ""
    tls:
      enabled: false

      ## Optional. Leave it blank if your Ingress Controller can provide a default certificate.
      secretName: ""

    hostname: ""
    path: "/"

  ## If set use existing secret with specified name to set pulsar admin credentials.
  existingSecretName:
  admin:
    user: pulsar
    password: pulsar

# These are jobs where job ttl configuration is used
# pulsar-helm-chart/charts/pulsar/templates/pulsar-cluster-initialize.yaml
# pulsar-helm-chart/charts/pulsar/templates/bookkeeper-cluster-initialize.yaml
job:
  ttl:
    enabled: false
    secondsAfterFinished: 3600

#############################################################
### Monitoring Stack : Prometheus / Grafana
#############################################################

configmapReload:
  prometheus:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true
    ## configmap-reload container name
    ##
    name: configmap-reload
    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}
    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []

    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true

    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
  alertmanager:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true

    ## configmap-reload container name
    ##
    name: configmap-reload
    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}
    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []

    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true

    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}

# Support the addition of arbitrary resources to be deployed
# This is simply an array of raw manifests
extraResources: []

